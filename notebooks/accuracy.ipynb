{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b86337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent working directory: /proj/sourasb-220503/IoT_attack_CL_IDS/results\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "cw = os.getcwd()\n",
    "pwd = os.path.abspath(os.path.join(cw, \"..\", \"results\"))\n",
    "print(f\"Parent working directory: {pwd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e90e0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def compute_performance_average(files, ddof=0):\n",
    "    \"\"\"\n",
    "    For each JSON file in `files`:\n",
    "      - Extract the last value from each key under 'performance_stability'\n",
    "      - Compute mean and std of those values for the file\n",
    "    Returns:\n",
    "      - file_means: list of per-file means\n",
    "      - file_stds:  list of per-file stds\n",
    "      - overall_perfile_mean, overall_perfile_std: stats over the per-file means\n",
    "      - overall_pooled_mean,  overall_pooled_std: stats over all last-values pooled\n",
    "    \"\"\"\n",
    "    def extract_last_values(fp):\n",
    "        with open(fp, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        perf_stab = data.get(\"performance_stability\", {})\n",
    "        last_vals = [v[-1] for v in perf_stab.values() if isinstance(v, list) and len(v) > 0]\n",
    "        if last_vals:\n",
    "            arr = np.asarray(last_vals, dtype=float)\n",
    "            return float(np.mean(arr)), float(np.std(arr, ddof=ddof)), arr\n",
    "        else:\n",
    "            return None, None, np.array([], dtype=float)\n",
    "\n",
    "    file_means, file_stds = [], []\n",
    "    pooled = []\n",
    "\n",
    "    for fp in files:\n",
    "        m, s, arr = extract_last_values(fp)\n",
    "        file_means.append(m)\n",
    "        file_stds.append(s)\n",
    "        if arr.size:\n",
    "            pooled.append(arr)\n",
    "\n",
    "    # Clean Nones for per-file aggregates\n",
    "    clean_means = [m for m in file_means if m is not None]\n",
    "\n",
    "    # overall_perfile_mean = float(np.mean(clean_means)) if clean_means else None\n",
    "    # overall_perfile_std  = float(np.std(clean_means, ddof=ddof)) if clean_means else None\n",
    "\n",
    "    # Pooled across all last-values\n",
    "    pooled_arr = np.concatenate(pooled) if pooled else np.array([], dtype=float)\n",
    "    overall_mean = float(np.mean(pooled_arr)) if pooled_arr.size else None\n",
    "    overall_std  = float(np.std(pooled_arr, ddof=ddof)) if pooled_arr.size else None\n",
    "\n",
    "    return file_means, file_stds, overall_mean, overall_std\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04ae51a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-file averages: [0.47463562962288924, 0.50152549732696, 0.4961553904708209]\n",
      "Per-file standard deviations: [0.2091139055011413, 0.19585770255590462, 0.19531869592913903]\n",
      "Overall avg±std : 0.4907721724735566±0.20053533283892663\n",
      "Overall avg±std : 0.49±0.20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files = [\n",
    "    pwd + \"/1_experiment_results_LSTM_WCL_random.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_WCL_random.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_WCL_random.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files)\n",
    "\n",
    "print(\"Per-file averages:\", file_means)\n",
    "print(\"Per-file standard deviations:\", file_stds)\n",
    "print(f\"Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aaeca9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-file averages: [0.5046047630307199, 0.521512437324695, 0.555772382476299]\n",
      "Per-file standard deviations: [0.19059189059559353, 0.1983586971434255, 0.21810898197239514]\n",
      "Overall avg±std : 0.5272965276105713±0.20379909690738002\n",
      "Overall avg±std : 0.53±0.20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files = [\n",
    "    pwd + \"/1_experiment_results_LSTM_WCL_b2w.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_WCL_b2w.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_WCL_b2w.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files)\n",
    "\n",
    "print(\"Per-file averages:\", file_means)\n",
    "print(\"Per-file standard deviations:\", file_stds)\n",
    "print(f\"Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f9ee02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-file averages: [0.5858279850326342, 0.5616453728354474, 0.6005275935427928]\n",
      "Per-file standard deviations: [0.2934041261003132, 0.2631176899563266, 0.29372278659931766]\n",
      "Overall avg±std : 0.5826669838036247±0.2842304679803771\n",
      "Overall avg±std : 0.58±0.28\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files = [\n",
    "    pwd + \"/1_experiment_results_LSTM_WCL_w2b.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_WCL_w2b.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_WCL_w2b.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files)\n",
    "\n",
    "print(\"Per-file averages:\", file_means)\n",
    "print(\"Per-file standard deviations:\", file_stds)\n",
    "print(f\"Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88f76b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-file averages: [0.582245449243203, 0.5845538452388884, 0.5885582354442725]\n",
      "Per-file standard deviations: [0.30646025794777815, 0.3062737939343609, 0.3020079311766125]\n",
      "Overall avg±std : 0.5851191766421213±0.3049320810061081\n",
      "Overall avg±std : 0.59±0.30\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "    pwd + \"/1_experiment_results_LSTM_WCL_toggle.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_WCL_toggle.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_WCL_toggle.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files)\n",
    "\n",
    "print(\"Per-file averages:\", file_means)\n",
    "print(\"Per-file standard deviations:\", file_stds)\n",
    "print(f\"Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a318f9",
   "metadata": {},
   "source": [
    "EWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dafd8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-file averages: [0.5717079492066316, 0.5944279227035163, 0.5758839485658321]\n",
      "Per-file standard deviations: [0.21970890784945396, 0.2566740216133694, 0.2180710359327708]\n",
      "Overall avg±std : 0.5806732734919933±0.23237974619151167\n",
      "Overall avg±std : 0.58±0.23\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files = [\n",
    "    pwd + \"/1_experiment_results_LSTM_EWC_random.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_EWC_random.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_EWC_random.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files)\n",
    "\n",
    "print(\"Per-file averages:\", file_means)\n",
    "print(\"Per-file standard deviations:\", file_stds)\n",
    "print(f\"Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0634667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-file averages: [0.5858279850326342, 0.6342014608050582, 0.564558949007367]\n",
      "Per-file standard deviations: [0.2934041261003132, 0.22853503956316343, 0.2304414528447301]\n",
      "Overall avg±std : 0.5948627982816865±0.2542734684091315\n",
      "Overall avg±std : 0.59±0.25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files = [\n",
    "    pwd + \"/1_experiment_results_LSTM_EWC_b2w.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_EWC_b2w.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_EWC_b2w.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files)\n",
    "\n",
    "print(\"Per-file averages:\", file_means)\n",
    "print(\"Per-file standard deviations:\", file_stds)\n",
    "print(f\"Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc5c7f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-file averages: [0.6204501526685241, 0.6320696048988216, 0.6476483780606966]\n",
      "Per-file standard deviations: [0.1868478157791697, 0.21994609398488987, 0.22860009664559358]\n",
      "Overall avg±std : 0.6333893785426807±0.21285274968310802\n",
      "Overall avg±std : 0.63±0.21\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files = [\n",
    "    pwd + \"/1_experiment_results_LSTM_EWC_w2b.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_EWC_w2b.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_EWC_w2b.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files)\n",
    "\n",
    "print(\"Per-file averages:\", file_means)\n",
    "print(\"Per-file standard deviations:\", file_stds)\n",
    "print(f\"Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1bc5bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-file averages: [0.6290402901586375, 0.631108759003066, 0.6407277317319854]\n",
      "Per-file standard deviations: [0.20978898355707085, 0.22653414937825872, 0.22551983350507523]\n",
      "Overall avg±std : 0.6336255936312296±0.22080619978515137\n",
      "Overall avg±std : 0.63±0.22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files = [\n",
    "    pwd + \"/1_experiment_results_LSTM_EWC_toggle.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_EWC_toggle.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_EWC_toggle.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files)\n",
    "\n",
    "print(\"Per-file averages:\", file_means)\n",
    "print(\"Per-file standard deviations:\", file_stds)\n",
    "print(f\"Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e9cdcb",
   "metadata": {},
   "source": [
    "## SI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce44ab59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-file averages: [0.5507650209346928, 0.5339300220183713, 0.5505636965665359]\n",
      "Per-file standard deviations: [0.18151068670546305, 0.20449813025470986, 0.1820375589439451]\n",
      "Overall avg±std : 0.5450862465065333±0.18981569992603117\n",
      "Overall avg±std : 0.55±0.19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files = [\n",
    "    pwd + \"/1_experiment_results_LSTM_SI_random.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_SI_random.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_SI_random.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files)\n",
    "\n",
    "print(\"Per-file averages:\", file_means)\n",
    "print(\"Per-file standard deviations:\", file_stds)\n",
    "print(f\"Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77c7a615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-file averages: [0.5721709186059861, 0.5295788421071385, 0.5811399018570027]\n",
      "Per-file standard deviations: [0.2571030473489144, 0.17898907729727898, 0.2794430776565301]\n",
      "Overall avg±std : 0.5609632208567091±0.24340993582329803\n",
      "Overall avg±std : 0.56±0.24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files = [\n",
    "    pwd + \"/1_experiment_results_LSTM_SI_b2w.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_SI_b2w.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_SI_b2w.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files)\n",
    "\n",
    "print(\"Per-file averages:\", file_means)\n",
    "print(\"Per-file standard deviations:\", file_stds)\n",
    "print(f\"Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d1ae052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-file averages: [0.6383447094037775, 0.5854918006890837, 0.6282956765746267]\n",
      "Per-file standard deviations: [0.23464470163487128, 0.24106223476999536, 0.24060047785880556]\n",
      "Overall avg±std : 0.6173773955558293±0.23988417708963752\n",
      "Overall avg±std : 0.62±0.24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files = [\n",
    "    pwd + \"/1_experiment_results_LSTM_SI_w2b.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_SI_w2b.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_SI_w2b.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files)\n",
    "\n",
    "print(\"Per-file averages:\", file_means)\n",
    "print(\"Per-file standard deviations:\", file_stds)\n",
    "print(f\"Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c663659f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-file averages: [0.6050539160321659, 0.615708005209903, 0.6311555293360179]\n",
      "Per-file standard deviations: [0.22120464846707105, 0.19769911569706217, 0.23899667684444736]\n",
      "Overall avg±std : 0.6173058168593624±0.22021226502714958\n",
      "Overall avg±std : 0.62±0.22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files = [\n",
    "    pwd + \"/1_experiment_results_LSTM_SI_toggle.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_SI_toggle.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_SI_toggle.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files)\n",
    "\n",
    "print(\"Per-file averages:\", file_means)\n",
    "print(\"Per-file standard deviations:\", file_stds)\n",
    "print(f\"Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bd62fc",
   "metadata": {},
   "source": [
    "## LwF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f23d055c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[random] Per-file averages: [0.350999732130844, 0.341222581509715, 0.30815874716445385]\n",
      "[random] Per-file standard deviations: [0.09707228019504162, 0.08234430486578263, 0.09797663506152952]\n",
      "[random] Overall avg±std : 0.3334603536016709±0.09453585988689446\n",
      "[random] Overall avg±std : 0.33±0.09\n",
      "[b2w] Per-file averages: [0.42317349664947934, 0.5091413717489243, 0.5069067253936671]\n",
      "[b2w] Per-file standard deviations: [0.1429354083828966, 0.1835696258833955, 0.17832940855825144]\n",
      "[b2w] Overall avg±std : 0.4797405312640236±0.1739079808433971\n",
      "[b2w] Overall avg±std : 0.48±0.17\n",
      "[w2b] Per-file averages: [0.5423424610839075, 0.585085121343076, 0.6102970306974458]\n",
      "[w2b] Per-file standard deviations: [0.27129130795392004, 0.29257758101604125, 0.26959185912430217]\n",
      "[w2b] Overall avg±std : 0.5792415377081431±0.2794282962054683\n",
      "[w2b] Overall avg±std : 0.58±0.28\n",
      "[toggle] Per-file averages: [0.5462790693605616, 0.566926976493204, 0.5750539768662121]\n",
      "[toggle] Per-file standard deviations: [0.2646054600014832, 0.2703467154649672, 0.29663024292421986]\n",
      "[toggle] Overall avg±std : 0.5627533409066593±0.27780870129435437\n",
      "[toggle] Overall avg±std : 0.56±0.28\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files_1 = [\n",
    "    pwd + \"/1_experiment_results_LSTM_LwF_random_alpha_1.0_T_4.0.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_LwF_random_alpha_1.0_T_4.0.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_LwF_random_alpha_1.0_T_4.0.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files_1)\n",
    "\n",
    "print(\"[random] Per-file averages:\", file_means)\n",
    "print(\"[random] Per-file standard deviations:\", file_stds)\n",
    "print(f\"[random] Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"[random] Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n",
    "\n",
    "\n",
    "files_2 = [\n",
    "    pwd + \"/1_experiment_results_LSTM_LwF_b2w_alpha_1.0_T_4.0.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_LwF_b2w_alpha_1.0_T_4.0.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_LwF_b2w_alpha_1.0_T_4.0.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files_2)\n",
    "\n",
    "print(\"[b2w] Per-file averages:\", file_means)\n",
    "print(\"[b2w] Per-file standard deviations:\", file_stds)\n",
    "print(f\"[b2w] Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"[b2w] Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n",
    "\n",
    "\n",
    "files_3 = [\n",
    "    pwd + \"/1_experiment_results_LSTM_LwF_w2b_alpha_1.0_T_4.0.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_LwF_w2b_alpha_1.0_T_4.0.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_LwF_w2b_alpha_1.0_T_4.0.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files_3)\n",
    "\n",
    "print(\"[w2b] Per-file averages:\", file_means)\n",
    "print(\"[w2b] Per-file standard deviations:\", file_stds)\n",
    "print(f\"[w2b] Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"[w2b] Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n",
    "\n",
    "\n",
    "files_4 = [\n",
    "    pwd + \"/1_experiment_results_LSTM_LwF_toggle_alpha_1.0_T_4.0.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_LwF_toggle_alpha_1.0_T_4.0.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_LwF_toggle_alpha_1.0_T_4.0.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files_4)\n",
    "\n",
    "print(\"[toggle] Per-file averages:\", file_means)\n",
    "print(\"[toggle] Per-file standard deviations:\", file_stds)\n",
    "print(f\"[toggle] Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"[toggle] Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d490e17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[random] Per-file averages: [0.7838978897570685, 0.764291818744525, 0.7708863801564384]\n",
      "[random] Per-file standard deviations: [0.1776528746954364, 0.18614846216463737, 0.1654820788899544]\n",
      "[random] Overall avg±std : 0.7730253628860108±0.17681928078172876\n",
      "[random] Overall avg±std : 0.77±0.18\n",
      "[b2w] Per-file averages: [0.7774659051202804, 0.7714156839956354, 0.7766201022221385]\n",
      "[b2w] Per-file standard deviations: [0.1787673096573466, 0.16995474097608848, 0.16002584904420025]\n",
      "[b2w] Overall avg±std : 0.7751672304460181±0.16977642767977486\n",
      "[b2w] Overall avg±std : 0.78±0.17\n",
      "[w2b] Per-file averages: [0.7756105810749983, 0.7789822329904726, 0.7905150581604339]\n",
      "[w2b] Per-file standard deviations: [0.17095000501966134, 0.17374980897860723, 0.1797127129805254]\n",
      "[w2b] Overall avg±std : 0.7817026240753017±0.1749587875788869\n",
      "[w2b] Overall avg±std : 0.78±0.17\n",
      "[toggle] Per-file averages: [0.7768473746199911, 0.7691638490903276, 0.778212820287357]\n",
      "[toggle] Per-file standard deviations: [0.18650867342202762, 0.18307651381860532, 0.17043479584546]\n",
      "[toggle] Overall avg±std : 0.7747413479992252±0.18018334160011903\n",
      "[toggle] Overall avg±std : 0.77±0.18\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files_1 = [\n",
    "    pwd + \"/1_experiment_results_LSTM_Replay_REPLAY_random.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_Replay_REPLAY_random.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_Replay_REPLAY_random.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files_1)\n",
    "\n",
    "print(\"[random] Per-file averages:\", file_means)\n",
    "print(\"[random] Per-file standard deviations:\", file_stds)\n",
    "print(f\"[random] Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"[random] Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n",
    "\n",
    "\n",
    "files_2 = [\n",
    "    pwd + \"/1_experiment_results_LSTM_Replay_REPLAY_b2w.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_Replay_REPLAY_b2w.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_Replay_REPLAY_b2w.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files_2)\n",
    "\n",
    "print(\"[b2w] Per-file averages:\", file_means)\n",
    "print(\"[b2w] Per-file standard deviations:\", file_stds)\n",
    "print(f\"[b2w] Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"[b2w] Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n",
    "\n",
    "\n",
    "files_3 = [\n",
    "    pwd + \"/1_experiment_results_LSTM_Replay_REPLAY_w2b.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_Replay_REPLAY_w2b.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_Replay_REPLAY_w2b.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files_3)\n",
    "\n",
    "print(\"[w2b] Per-file averages:\", file_means)\n",
    "print(\"[w2b] Per-file standard deviations:\", file_stds)\n",
    "print(f\"[w2b] Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"[w2b] Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n",
    "\n",
    "\n",
    "files_4 = [\n",
    "    pwd + \"/1_experiment_results_LSTM_Replay_REPLAY_toggle.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_Replay_REPLAY_toggle.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_Replay_REPLAY_toggle.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files_4)\n",
    "\n",
    "print(\"[toggle] Per-file averages:\", file_means)\n",
    "print(\"[toggle] Per-file standard deviations:\", file_stds)\n",
    "print(f\"[toggle] Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"[toggle] Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dcbe08c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[random] Per-file averages: [0.5018843808538107, 0.45013126032924405, 0.5152544410634374]\n",
      "[random] Per-file standard deviations: [0.20348759573135322, 0.1883496791836308, 0.22112934547142654]\n",
      "[random] Overall avg±std : 0.48909002741549745±0.20667772678581078\n",
      "[random] Overall avg±std : 0.49±0.21\n",
      "[b2w] Per-file averages: [0.47328082737079563, 0.46322668651407, 0.5169291298113883]\n",
      "[b2w] Per-file standard deviations: [0.2104618200957648, 0.1631433107334366, 0.18204581531466937]\n",
      "[b2w] Overall avg±std : 0.4844788812320847±0.18768827636282345\n",
      "[b2w] Overall avg±std : 0.48±0.19\n",
      "[w2b] Per-file averages: [0.5792466914937505, 0.5970834433920374, 0.591059810920628]\n",
      "[w2b] Per-file standard deviations: [0.28294888127227164, 0.2804555847112314, 0.2715129826256461]\n",
      "[w2b] Overall avg±std : 0.589129981935472±0.2784477002731582\n",
      "[w2b] Overall avg±std : 0.59±0.28\n",
      "[toggle] Per-file averages: [0.592103009758471, 0.5674100219294941, 0.580043834661037]\n",
      "[toggle] Per-file standard deviations: [0.29332335138157734, 0.2794810961227267, 0.3077398199622215]\n",
      "[toggle] Overall avg±std : 0.5798522887830007±0.29391438390839697\n",
      "[toggle] Overall avg±std : 0.58±0.29\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files_1 = [\n",
    "    pwd + \"/1_experiment_results_LSTM_GR_random.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_GR_random.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_GR_random.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files_1)\n",
    "\n",
    "print(\"[random] Per-file averages:\", file_means)\n",
    "print(\"[random] Per-file standard deviations:\", file_stds)\n",
    "print(f\"[random] Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"[random] Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n",
    "\n",
    "\n",
    "files_2 = [\n",
    "    pwd + \"/1_experiment_results_LSTM_GR_b2w.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_GR_b2w.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_GR_b2w.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files_2)\n",
    "\n",
    "print(\"[b2w] Per-file averages:\", file_means)\n",
    "print(\"[b2w] Per-file standard deviations:\", file_stds)\n",
    "print(f\"[b2w] Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"[b2w] Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n",
    "\n",
    "\n",
    "files_3 = [\n",
    "    pwd + \"/1_experiment_results_LSTM_GR_w2b.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_GR_w2b.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_GR_w2b.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files_3)\n",
    "\n",
    "print(\"[w2b] Per-file averages:\", file_means)\n",
    "print(\"[w2b] Per-file standard deviations:\", file_stds)\n",
    "print(f\"[w2b] Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"[w2b] Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n",
    "\n",
    "\n",
    "files_4 = [\n",
    "    pwd + \"/1_experiment_results_LSTM_GR_toggle.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_GR_toggle.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_GR_toggle.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files_4)\n",
    "\n",
    "print(\"[toggle] Per-file averages:\", file_means)\n",
    "print(\"[toggle] Per-file standard deviations:\", file_stds)\n",
    "print(f\"[toggle] Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"[toggle] Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7290f4e8",
   "metadata": {},
   "source": [
    "## Groupped Performance F1-Score / Auc-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8264179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_grouped_performance_average(\n",
    "    files,\n",
    "    ddof=0,\n",
    "    groups=(\"blackhole\", \"disflooding\", \"localrepair\", \"worstparent\"),\n",
    "    case_insensitive=True,\n",
    "):\n",
    "\n",
    "    # Normalize group names once\n",
    "    group_list = list(groups)\n",
    "    group_norm = [g.lower() for g in group_list]\n",
    "\n",
    "    per_file_results = []\n",
    "    pooled_buckets = {g: [] for g in group_list}\n",
    "\n",
    "    for fp in files:\n",
    "        with open(fp, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # perf_stab = data.get(\"roc_auc_stability\", {}) or {}\n",
    "        perf_stab = data.get(\"performance_stability\", {}) or {}\n",
    "        # Buckets of final values per group for this file\n",
    "        file_buckets = {g: [] for g in group_list}\n",
    "\n",
    "        for k, v in perf_stab.items():\n",
    "            if not (isinstance(v, list) and len(v) > 0):\n",
    "                continue\n",
    "            k_cmp = k.lower() if case_insensitive else k\n",
    "            last_val = v[-1]\n",
    "\n",
    "            # Find which group this key belongs to (first matching prefix wins)\n",
    "            for g_name, g_norm in zip(group_list, group_norm):\n",
    "                if k_cmp.startswith(g_norm):\n",
    "                    try:\n",
    "                        file_buckets[g_name].append(float(last_val))\n",
    "                    except Exception:\n",
    "                        # Skip if conversion to float fails\n",
    "                        pass\n",
    "                    break  # stop at first matched group\n",
    "\n",
    "        # Compute per-file stats\n",
    "        file_group_stats = {}\n",
    "        for g in group_list:\n",
    "            arr = np.asarray(file_buckets[g], dtype=float)\n",
    "            if arr.size > 0:\n",
    "                mean_g = float(np.mean(arr))\n",
    "                std_g = float(np.std(arr, ddof=ddof))\n",
    "            else:\n",
    "                mean_g, std_g = None, None\n",
    "            file_group_stats[g] = {\n",
    "                \"count\": int(arr.size),\n",
    "                \"mean\": mean_g,\n",
    "                \"std\": std_g,\n",
    "                \"values\": arr,\n",
    "            }\n",
    "\n",
    "            # Add to pooled buckets\n",
    "            if arr.size:\n",
    "                pooled_buckets[g].append(arr)\n",
    "\n",
    "        per_file_results.append({\n",
    "            \"file\": fp,\n",
    "            \"groups\": file_group_stats\n",
    "        })\n",
    "\n",
    "    # Pooled stats across files per group\n",
    "    pooled_stats = {}\n",
    "    for g in group_list:\n",
    "        if len(pooled_buckets[g]) > 0:\n",
    "            pooled_arr = np.concatenate(pooled_buckets[g], axis=0)\n",
    "        else:\n",
    "            pooled_arr = np.asarray([], dtype=float)\n",
    "\n",
    "        if pooled_arr.size > 0:\n",
    "            pooled_mean = float(np.mean(pooled_arr))\n",
    "            pooled_std  = float(np.std(pooled_arr, ddof=ddof))\n",
    "        else:\n",
    "            pooled_mean, pooled_std = None, None\n",
    "\n",
    "        pooled_stats[g] = {\n",
    "            \"count\": int(pooled_arr.size),\n",
    "            \"mean\": pooled_mean,\n",
    "            \"std\": pooled_std,\n",
    "            \"values\": pooled_arr,\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"per_file\": per_file_results,\n",
    "        \"pooled\": pooled_stats\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd456aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Random Ordering ===\n",
      "                   Blackhole  Disflooding  LocalRepair  WorstParent\n",
      "WCL                     0.31         0.58         0.62         0.47\n",
      "EWC                     0.56         0.77         0.56         0.46\n",
      "SI                      0.34         0.60         0.55         0.52\n",
      "LwF                     0.30         0.33         0.36         0.43\n",
      "Replay                  0.65         0.97         0.87         0.64\n",
      "Generative Replay       0.27         0.59         0.54         0.43\n",
      "\n",
      "=== Best-to-Worst Ordering ===\n",
      "                   Blackhole  Disflooding  LocalRepair  WorstParent\n",
      "WCL                     0.32         0.58         0.61         0.48\n",
      "EWC                     0.45         0.84         0.64         0.49\n",
      "SI                      0.38         0.96         0.62         0.47\n",
      "LwF                     0.34         0.58         0.62         0.47\n",
      "Replay                  0.64         0.96         0.88         0.62\n",
      "Generative Replay       0.31         0.53         0.51         0.44\n",
      "\n",
      "=== Worst-to-Best Ordering ===\n",
      "                   Blackhole  Disflooding  LocalRepair  WorstParent\n",
      "WCL                     0.28         0.98         0.64         0.44\n",
      "EWC                     0.52         0.83         0.63         0.49\n",
      "SI                      0.49         0.99         0.53         0.42\n",
      "LwF                     0.36         0.98         0.57         0.41\n",
      "Replay                  0.64         0.97         0.88         0.62\n",
      "Generative Replay       0.34         0.97         0.65         0.40\n",
      "\n",
      "=== Toggle Ordering ===\n",
      "                   Blackhole  Disflooding  LocalRepair  WorstParent\n",
      "WCL                     0.29         0.97         0.65         0.41\n",
      "EWC                     0.53         0.84         0.68         0.47\n",
      "SI                      0.48         0.99         0.63         0.46\n",
      "LwF                     0.34         0.94         0.64         0.40\n",
      "Replay                  0.63         0.97         0.88         0.62\n",
      "Generative Replay       0.27         0.98         0.67         0.41\n",
      "\n",
      "=== Averaged Across Orderings ===\n",
      "                   Blackhole  Disflooding  LocalRepair  WorstParent\n",
      "EWC                     0.52         0.82         0.63         0.48\n",
      "Generative Replay       0.30         0.77         0.59         0.42\n",
      "LwF                     0.33         0.71         0.54         0.43\n",
      "Replay                  0.64         0.97         0.88         0.63\n",
      "SI                      0.42         0.88         0.58         0.47\n",
      "WCL                     0.30         0.78         0.63         0.45\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ATTACK_KEYS = [\n",
    "    (\"blackhole\", \"Blackhole\"),\n",
    "    (\"disflooding\", \"Disflooding\"),\n",
    "    (\"localrepair\", \"LocalRepair\"),\n",
    "    (\"worstparent\", \"WorstParent\"),\n",
    "]\n",
    "\n",
    "def methods_for(ordering_suffix: str):\n",
    "    \"\"\"Return the methods→filelist dict for a given ordering suffix.\"\"\"\n",
    "    return {\n",
    "        \"WCL\": [f\"{pwd}/1_experiment_results_LSTM_WCL_{ordering_suffix}.json\",\n",
    "                f\"{pwd}/2_experiment_results_LSTM_WCL_{ordering_suffix}.json\",\n",
    "                f\"{pwd}/3_experiment_results_LSTM_WCL_{ordering_suffix}.json\"],\n",
    "\n",
    "        \"EWC\": [f\"{pwd}/1_experiment_results_LSTM_EWC_{ordering_suffix}.json\",\n",
    "                f\"{pwd}/2_experiment_results_LSTM_EWC_{ordering_suffix}.json\",\n",
    "                f\"{pwd}/3_experiment_results_LSTM_EWC_{ordering_suffix}.json\"],\n",
    "\n",
    "        \"SI\":  [f\"{pwd}/1_experiment_results_LSTM_SI_{ordering_suffix}.json\",\n",
    "                f\"{pwd}/2_experiment_results_LSTM_SI_{ordering_suffix}.json\",\n",
    "                f\"{pwd}/3_experiment_results_LSTM_SI_{ordering_suffix}.json\"],\n",
    "\n",
    "        \"LwF\": [f\"{pwd}/1_experiment_results_LSTM_LwF_{ordering_suffix}_alpha_1.0_T_4.0.json\",\n",
    "                f\"{pwd}/2_experiment_results_LSTM_LwF_{ordering_suffix}_alpha_1.0_T_4.0.json\",\n",
    "                f\"{pwd}/3_experiment_results_LSTM_LwF_{ordering_suffix}_alpha_1.0_T_4.0.json\"],\n",
    "\n",
    "        \"Replay\": [f\"{pwd}/1_experiment_results_LSTM_Replay_REPLAY_{ordering_suffix}.json\",\n",
    "                   f\"{pwd}/2_experiment_results_LSTM_Replay_REPLAY_{ordering_suffix}.json\",\n",
    "                   f\"{pwd}/3_experiment_results_LSTM_Replay_REPLAY_{ordering_suffix}.json\"],\n",
    "\n",
    "        \"Generative Replay\": [f\"{pwd}/1_experiment_results_LSTM_GR_{ordering_suffix}.json\",\n",
    "                              f\"{pwd}/2_experiment_results_LSTM_GR_{ordering_suffix}.json\",\n",
    "                              f\"{pwd}/3_experiment_results_LSTM_GR_{ordering_suffix}.json\"],\n",
    "    }\n",
    "\n",
    "def build_numeric_df(methods_dict):\n",
    "    \"\"\"\n",
    "    Returns a numeric DataFrame (floats) with rows=methods and columns=[Blackhole, Disflooding, LocalRepair, WorstParent].\n",
    "    Missing values become NaN (so we can average later).\n",
    "    \"\"\"\n",
    "    rows = {}\n",
    "    for method, files in methods_dict.items():\n",
    "        results = compute_grouped_performance_average(files, ddof=0)\n",
    "        pooled = results[\"pooled\"]\n",
    "        row = {}\n",
    "        for key, disp in ATTACK_KEYS:\n",
    "            m = pooled.get(key, {}).get(\"mean\", None)\n",
    "            row[disp] = float(m) if m is not None else np.nan\n",
    "        rows[method] = row\n",
    "    return pd.DataFrame.from_dict(rows, orient=\"index\")[ [disp for _, disp in ATTACK_KEYS] ]\n",
    "\n",
    "def format_df_for_print(df):\n",
    "    \"\"\"Format to 2 decimals with '--' for NaN, leaving a copy for printing/LaTeX.\"\"\"\n",
    "    out = df.copy()\n",
    "    out = out.round(2)\n",
    "    return out.fillna(\"--\")\n",
    "\n",
    "def to_latex(df):\n",
    "    \"\"\"Pretty LaTeX (bold method names).\"\"\"\n",
    "    # Ensure strings for NaN handled:\n",
    "    display_df = df.copy()\n",
    "    return display_df.to_latex(index=True, header=True, escape=False, bold_rows=True)\n",
    "\n",
    "# ---- Build each ordering table (numeric) ----\n",
    "orderings = {\n",
    "    \"Random\": build_numeric_df(methods_for(\"random\")),\n",
    "    \"Best-to-Worst\": build_numeric_df(methods_for(\"b2w\")),\n",
    "    \"Worst-to-Best\": build_numeric_df(methods_for(\"w2b\")),\n",
    "    \"Toggle\": build_numeric_df(methods_for(\"toggle\")),\n",
    "}\n",
    "\n",
    "# ---- Print the 4 individual tables (pretty + LaTeX) ----\n",
    "for name, numeric_df in orderings.items():\n",
    "    print(f\"\\n=== {name} Ordering ===\")\n",
    "    pretty = format_df_for_print(numeric_df)\n",
    "    print(pretty)\n",
    "    #print(to_latex(pretty))\n",
    "\n",
    "# ---- Averaged table across the four orderings ----\n",
    "# Concatenate vertically then mean by method name (row index)\n",
    "all_concat = pd.concat(orderings.values(), axis=0, keys=orderings.keys())  # MultiIndex (ordering, method)\n",
    "avg_numeric = all_concat.groupby(level=1).mean(numeric_only=True)  # average across the 4 orderings by method\n",
    "\n",
    "print(\"\\n=== Averaged Across Orderings ===\")\n",
    "avg_pretty = format_df_for_print(avg_numeric)\n",
    "print(avg_pretty)\n",
    "#print(to_latex(avg_pretty))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08a65268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################\n",
      "### Metric: ROC-AUC Stability (roc_auc_stability)\n",
      "############################\n",
      "\n",
      "=== ROC-AUC Stability — Random Ordering ===\n",
      "                   Blackhole  Disflooding  LocalRepair  WorstParent\n",
      "WCL                     0.47         0.90         0.78         0.59\n",
      "Replay                  0.70         1.00         0.95         0.70\n",
      "EWC                     0.71         1.00         0.64         0.57\n",
      "SI                      0.56         1.00         0.68         0.59\n",
      "LwF                     0.49         0.44         0.62         0.61\n",
      "Generative Replay       0.44         0.79         0.68         0.53\n",
      "\n",
      "=== ROC-AUC Stability — Best-to-Worst Ordering ===\n",
      "                   Blackhole  Disflooding  LocalRepair  WorstParent\n",
      "WCL                     0.45         0.95         0.78         0.63\n",
      "Replay                  0.69         1.00         0.95         0.69\n",
      "EWC                     0.56         1.00         0.80         0.61\n",
      "SI                      0.56         1.00         0.82         0.58\n",
      "LwF                     0.52         0.81         0.80         0.57\n",
      "Generative Replay       0.45         0.99         0.73         0.56\n",
      "\n",
      "=== ROC-AUC Stability — Worst-to-Best Ordering ===\n",
      "                   Blackhole  Disflooding  LocalRepair  WorstParent\n",
      "WCL                     0.54         1.00         0.77         0.58\n",
      "Replay                  0.69         0.99         0.95         0.69\n",
      "EWC                     0.62         1.00         0.72         0.58\n",
      "SI                      0.67         1.00         0.76         0.61\n",
      "LwF                     0.57         0.99         0.65         0.54\n",
      "Generative Replay       0.54         1.00         0.70         0.59\n",
      "\n",
      "=== ROC-AUC Stability — Toggle Ordering ===\n",
      "                   Blackhole  Disflooding  LocalRepair  WorstParent\n",
      "WCL                     0.51         0.99         0.68         0.48\n",
      "Replay                  0.68         1.00         0.95         0.70\n",
      "EWC                     0.67         1.00         0.79         0.56\n",
      "SI                      0.66         1.00         0.83         0.59\n",
      "LwF                     0.49         0.99         0.82         0.59\n",
      "Generative Replay       0.58         1.00         0.95         0.63\n",
      "\n",
      "=== ROC-AUC Stability — Averaged Across Orderings ===\n",
      "                   Blackhole  Disflooding  LocalRepair  WorstParent\n",
      "WCL                     0.49         0.96         0.75         0.57\n",
      "Replay                  0.69         1.00         0.95         0.70\n",
      "EWC                     0.64         1.00         0.74         0.58\n",
      "SI                      0.61         1.00         0.77         0.59\n",
      "LwF                     0.52         0.81         0.72         0.58\n",
      "Generative Replay       0.50         0.94         0.77         0.58\n",
      "\n",
      "\n",
      "############################\n",
      "### Metric: Performance Stability (performance_stability)\n",
      "############################\n",
      "\n",
      "=== Performance Stability — Random Ordering ===\n",
      "                   Blackhole  Disflooding  LocalRepair  WorstParent\n",
      "WCL                     0.31         0.58         0.62         0.47\n",
      "Replay                  0.65         0.97         0.87         0.64\n",
      "EWC                     0.56         0.77         0.56         0.46\n",
      "SI                      0.34         0.60         0.55         0.52\n",
      "LwF                     0.30         0.33         0.36         0.43\n",
      "Generative Replay       0.27         0.59         0.54         0.43\n",
      "\n",
      "=== Performance Stability — Best-to-Worst Ordering ===\n",
      "                   Blackhole  Disflooding  LocalRepair  WorstParent\n",
      "WCL                     0.32         0.58         0.61         0.48\n",
      "Replay                  0.64         0.96         0.88         0.62\n",
      "EWC                     0.45         0.84         0.64         0.49\n",
      "SI                      0.38         0.96         0.62         0.47\n",
      "LwF                     0.34         0.58         0.62         0.47\n",
      "Generative Replay       0.31         0.53         0.51         0.44\n",
      "\n",
      "=== Performance Stability — Worst-to-Best Ordering ===\n",
      "                   Blackhole  Disflooding  LocalRepair  WorstParent\n",
      "WCL                     0.28         0.98         0.64         0.44\n",
      "Replay                  0.64         0.97         0.88         0.62\n",
      "EWC                     0.52         0.83         0.63         0.49\n",
      "SI                      0.49         0.99         0.53         0.42\n",
      "LwF                     0.36         0.98         0.57         0.41\n",
      "Generative Replay       0.34         0.97         0.65         0.40\n",
      "\n",
      "=== Performance Stability — Toggle Ordering ===\n",
      "                   Blackhole  Disflooding  LocalRepair  WorstParent\n",
      "WCL                     0.29         0.97         0.65         0.41\n",
      "Replay                  0.63         0.97         0.88         0.62\n",
      "EWC                     0.53         0.84         0.68         0.47\n",
      "SI                      0.48         0.99         0.63         0.46\n",
      "LwF                     0.34         0.94         0.64         0.40\n",
      "Generative Replay       0.27         0.98         0.67         0.41\n",
      "\n",
      "=== Performance Stability — Averaged Across Orderings ===\n",
      "                   Blackhole  Disflooding  LocalRepair  WorstParent\n",
      "WCL                     0.30         0.78         0.63         0.45\n",
      "Replay                  0.64         0.97         0.88         0.63\n",
      "EWC                     0.52         0.82         0.63         0.48\n",
      "SI                      0.42         0.88         0.58         0.47\n",
      "LwF                     0.33         0.71         0.54         0.43\n",
      "Generative Replay       0.30         0.77         0.59         0.42\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# =========================================\n",
    "# Config\n",
    "# =========================================\n",
    "# Column order (display names)\n",
    "ATTACK_KEYS = [\n",
    "    (\"blackhole\", \"Blackhole\"),\n",
    "    (\"disflooding\", \"Disflooding\"),\n",
    "    (\"localrepair\", \"LocalRepair\"),\n",
    "    (\"worstparent\", \"WorstParent\"),\n",
    "]\n",
    "\n",
    "# Two metrics ⇒ 5 tables each (4 orderings + averaged) = 10 total\n",
    "METRIC_KEYS = [\n",
    "    (\"roc_auc_stability\", \"ROC-AUC Stability\"),\n",
    "    (\"performance_stability\", \"Performance Stability\"),\n",
    "]\n",
    "\n",
    "# Orderings: suffix used in filenames → label used in prints\n",
    "ORDERING_LABELS = [\n",
    "    (\"random\", \"Random\"),\n",
    "    (\"b2w\", \"Best-to-Worst\"),\n",
    "    (\"w2b\", \"Worst-to-Best\"),\n",
    "    (\"toggle\", \"Toggle\"),\n",
    "]\n",
    "\n",
    "# Fixed row order\n",
    "ROW_ORDER = [\"WCL\", \"Replay\", \"EWC\", \"SI\", \"LwF\", \"Generative Replay\"]\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# Helpers\n",
    "# =========================================\n",
    "def compute_grouped_performance_average(\n",
    "    files,\n",
    "    ddof=0,\n",
    "    groups=(\"blackhole\", \"disflooding\", \"localrepair\", \"worstparent\"),\n",
    "    case_insensitive=True,\n",
    "    metric_key=\"performance_stability\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Aggregate last-step values for a metric across JSON files by attack-group prefix.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"per_file\": [ { \"file\": <path>, \"groups\": { g: {count,mean,std,values} } }, ... ],\n",
    "          \"pooled\":   { g: {count,mean,std,values} }\n",
    "        }\n",
    "    \"\"\"\n",
    "    group_list = list(groups)\n",
    "    group_norm = [g.lower() for g in group_list]\n",
    "\n",
    "    per_file_results = []\n",
    "    pooled_buckets = {g: [] for g in group_list}\n",
    "\n",
    "    for fp in files:\n",
    "        with open(fp, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        perf_stab = data.get(metric_key, {}) or {}\n",
    "        file_buckets = {g: [] for g in group_list}\n",
    "\n",
    "        for k, v in perf_stab.items():\n",
    "            if not (isinstance(v, list) and len(v) > 0):\n",
    "                continue\n",
    "            k_cmp = k.lower() if case_insensitive else k\n",
    "            last_val = v[-1]\n",
    "\n",
    "            for g_name, g_norm in zip(group_list, group_norm):\n",
    "                if k_cmp.startswith(g_norm):\n",
    "                    try:\n",
    "                        file_buckets[g_name].append(float(last_val))\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    break\n",
    "\n",
    "        file_group_stats = {}\n",
    "        for g in group_list:\n",
    "            arr = np.asarray(file_buckets[g], dtype=float)\n",
    "            if arr.size > 0:\n",
    "                mean_g = float(np.mean(arr))\n",
    "                std_g = float(np.std(arr, ddof=ddof))\n",
    "            else:\n",
    "                mean_g, std_g = None, None\n",
    "            file_group_stats[g] = {\n",
    "                \"count\": int(arr.size),\n",
    "                \"mean\": mean_g,\n",
    "                \"std\": std_g,\n",
    "                \"values\": arr,\n",
    "            }\n",
    "            if arr.size:\n",
    "                pooled_buckets[g].append(arr)\n",
    "\n",
    "        per_file_results.append({\n",
    "            \"file\": fp,\n",
    "            \"groups\": file_group_stats\n",
    "        })\n",
    "\n",
    "    pooled_stats = {}\n",
    "    for g in group_list:\n",
    "        if len(pooled_buckets[g]) > 0:\n",
    "            pooled_arr = np.concatenate(pooled_buckets[g], axis=0)\n",
    "        else:\n",
    "            pooled_arr = np.asarray([], dtype=float)\n",
    "\n",
    "        if pooled_arr.size > 0:\n",
    "            pooled_mean = float(np.mean(pooled_arr))\n",
    "            pooled_std  = float(np.std(pooled_arr, ddof=ddof))\n",
    "        else:\n",
    "            pooled_mean, pooled_std = None, None\n",
    "\n",
    "        pooled_stats[g] = {\n",
    "            \"count\": int(pooled_arr.size),\n",
    "            \"mean\": pooled_mean,\n",
    "            \"std\": pooled_std,\n",
    "            \"values\": pooled_arr,\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"per_file\": per_file_results,\n",
    "        \"pooled\": pooled_stats\n",
    "    }\n",
    "\n",
    "\n",
    "def methods_for(ordering_suffix: str):\n",
    "    \"\"\"Return the methods→filelist dict for a given ordering suffix.\"\"\"\n",
    "    return {\n",
    "        \"WCL\": [f\"{pwd}/1_experiment_results_LSTM_WCL_{ordering_suffix}.json\",\n",
    "                f\"{pwd}/2_experiment_results_LSTM_WCL_{ordering_suffix}.json\",\n",
    "                f\"{pwd}/3_experiment_results_LSTM_WCL_{ordering_suffix}.json\"],\n",
    "\n",
    "        \"Replay\": [f\"{pwd}/1_experiment_results_LSTM_Replay_REPLAY_{ordering_suffix}.json\",\n",
    "                   f\"{pwd}/2_experiment_results_LSTM_Replay_REPLAY_{ordering_suffix}.json\",\n",
    "                   f\"{pwd}/3_experiment_results_LSTM_Replay_REPLAY_{ordering_suffix}.json\"],\n",
    "\n",
    "        \"EWC\": [f\"{pwd}/1_experiment_results_LSTM_EWC_{ordering_suffix}.json\",\n",
    "                f\"{pwd}/2_experiment_results_LSTM_EWC_{ordering_suffix}.json\",\n",
    "                f\"{pwd}/3_experiment_results_LSTM_EWC_{ordering_suffix}.json\"],\n",
    "\n",
    "        \"SI\":  [f\"{pwd}/1_experiment_results_LSTM_SI_{ordering_suffix}.json\",\n",
    "                f\"{pwd}/2_experiment_results_LSTM_SI_{ordering_suffix}.json\",\n",
    "                f\"{pwd}/3_experiment_results_LSTM_SI_{ordering_suffix}.json\"],\n",
    "\n",
    "        \"LwF\": [f\"{pwd}/1_experiment_results_LSTM_LwF_{ordering_suffix}_alpha_1.0_T_4.0.json\",\n",
    "                f\"{pwd}/2_experiment_results_LSTM_LwF_{ordering_suffix}_alpha_1.0_T_4.0.json\",\n",
    "                f\"{pwd}/3_experiment_results_LSTM_LwF_{ordering_suffix}_alpha_1.0_T_4.0.json\"],\n",
    "\n",
    "        \"Generative Replay\": [f\"{pwd}/1_experiment_results_LSTM_GR_{ordering_suffix}.json\",\n",
    "                              f\"{pwd}/2_experiment_results_LSTM_GR_{ordering_suffix}.json\",\n",
    "                              f\"{pwd}/3_experiment_results_LSTM_GR_{ordering_suffix}.json\"],\n",
    "    }\n",
    "\n",
    "\n",
    "def build_numeric_df(methods_dict, metric_key: str):\n",
    "    \"\"\"\n",
    "    Returns a numeric DataFrame (floats) with rows=methods and columns=[Blackhole, Disflooding, LocalRepair, WorstParent],\n",
    "    computed from the specified metric_key.\n",
    "    \"\"\"\n",
    "    rows = {}\n",
    "    for method, files in methods_dict.items():\n",
    "        results = compute_grouped_performance_average(files, ddof=0, metric_key=metric_key)\n",
    "        pooled = results[\"pooled\"]\n",
    "        row = {}\n",
    "        for key, disp in ATTACK_KEYS:\n",
    "            m = pooled.get(key, {}).get(\"mean\", None)\n",
    "            row[disp] = float(m) if m is not None else np.nan\n",
    "        rows[method] = row\n",
    "    # enforce column order\n",
    "    df = pd.DataFrame.from_dict(rows, orient=\"index\")[ [disp for _, disp in ATTACK_KEYS] ]\n",
    "    return df\n",
    "\n",
    "\n",
    "def format_df_for_print(df):\n",
    "    \"\"\"Format to 2 decimals with '--' for NaN, enforce row order.\"\"\"\n",
    "    df_ordered = df.reindex(ROW_ORDER)\n",
    "    out = df_ordered.copy().round(2)\n",
    "    return out.fillna(\"--\")\n",
    "\n",
    "\n",
    "def to_latex(df):\n",
    "    \"\"\"Pretty LaTeX (bold method names) with fixed row order.\"\"\"\n",
    "    df_ordered = df.reindex(ROW_ORDER)\n",
    "    return df_ordered.to_latex(index=True, header=True, escape=False, bold_rows=True)\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# Main: build & print 10 tables\n",
    "# =========================================\n",
    "for metric_key, metric_title in METRIC_KEYS:\n",
    "    print(f\"\\n\\n############################\")\n",
    "    print(f\"### Metric: {metric_title} ({metric_key})\")\n",
    "    print(f\"############################\")\n",
    "\n",
    "    # Build numeric tables per ordering for this metric\n",
    "    orderings = {}\n",
    "    for suf, label in ORDERING_LABELS:\n",
    "        orderings[label] = build_numeric_df(methods_for(suf), metric_key=metric_key)\n",
    "\n",
    "    # 4 individual tables (with fixed row order)\n",
    "    for _, label in ORDERING_LABELS:\n",
    "        numeric_df = orderings[label].reindex(ROW_ORDER)\n",
    "        pretty = format_df_for_print(numeric_df)\n",
    "        print(f\"\\n=== {metric_title} — {label} Ordering ===\")\n",
    "        print(pretty)\n",
    "        # Uncomment if you want LaTeX too:\n",
    "        # print(to_latex(pretty))\n",
    "\n",
    "    # Averaged table across the four orderings (5th table)\n",
    "    all_concat = pd.concat(orderings.values(), axis=0, keys=orderings.keys())  # MultiIndex (ordering, method)\n",
    "    avg_numeric = all_concat.groupby(level=1).mean(numeric_only=True).reindex(ROW_ORDER)\n",
    "\n",
    "    avg_pretty = format_df_for_print(avg_numeric)\n",
    "    print(f\"\\n=== {metric_title} — Averaged Across Orderings ===\")\n",
    "    print(avg_pretty)\n",
    "    # Uncomment if you want LaTeX too:\n",
    "    # print(to_latex(avg_pretty))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3887424",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "files_1 = [\n",
    "    pwd + \"/1_experiment_results_LSTM_GR_random.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_GR_random.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_GR_random.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files_1)\n",
    "\n",
    "print(\"[random] Per-file averages:\", file_means)\n",
    "print(\"[random] Per-file standard deviations:\", file_stds)\n",
    "print(f\"[random] Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"[random] Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n",
    "\n",
    "\n",
    "files_2 = [\n",
    "    pwd + \"/1_experiment_results_LSTM_GR_b2w.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_GR_b2w.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_GR_b2w.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files_2)\n",
    "\n",
    "print(\"[b2w] Per-file averages:\", file_means)\n",
    "print(\"[b2w] Per-file standard deviations:\", file_stds)\n",
    "print(f\"[b2w] Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"[b2w] Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n",
    "\n",
    "\n",
    "files_3 = [\n",
    "    pwd + \"/1_experiment_results_LSTM_GR_w2b.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_GR_w2b.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_GR_w2b.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files_3)\n",
    "\n",
    "print(\"[w2b] Per-file averages:\", file_means)\n",
    "print(\"[w2b] Per-file standard deviations:\", file_stds)\n",
    "print(f\"[w2b] Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"[w2b] Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")\n",
    "\n",
    "\n",
    "files_4 = [\n",
    "    pwd + \"/1_experiment_results_LSTM_GR_toggle.json\",\n",
    "    pwd + \"/2_experiment_results_LSTM_GR_toggle.json\",\n",
    "    pwd + \"/3_experiment_results_LSTM_GR_toggle.json\"\n",
    "]\n",
    "\n",
    "file_means, file_stds, overall_mean, overall_std = compute_performance_average(files_4)\n",
    "\n",
    "print(\"[toggle] Per-file averages:\", file_means)\n",
    "print(\"[toggle] Per-file standard deviations:\", file_stds)\n",
    "print(f\"[toggle] Overall avg±std : {overall_mean}±{overall_std}\")\n",
    "print(f\"[toggle] Overall avg±std : {overall_mean:.2f}±{overall_std:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personalized_fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
