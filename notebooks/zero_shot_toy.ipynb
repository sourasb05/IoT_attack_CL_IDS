{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b4a66d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ld/f2yvzx4155lc1np6yd4rhdrm0000gn/T/ipykernel_25991/1508556768.py:8: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Epoch 5, Loss: -0.1586\n",
      "Epoch 10, Loss: -0.3182\n",
      "Epoch 15, Loss: -0.4360\n",
      "Epoch 20, Loss: -0.5192\n",
      "\n",
      "Zero-Shot Accuracy on Dataset B (with unseen class 2!): 0.4560\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# â”€â”€â”€ DEVICE SELECTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# â”€â”€â”€ SIMULATE DATA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Dataset A â†’ classes 0 and 1\n",
    "X_A = torch.randn(1000, 14)\n",
    "y_A = torch.randint(0, 2, (1000,))\n",
    "\n",
    "# Dataset B â†’ classes 0 and 2\n",
    "X_B = torch.randn(500, 14) + 1.0  # shift distribution\n",
    "y_B = torch.randint(0, 2, (500,))\n",
    "y_B = y_B * 2  # labels 0 and 2\n",
    "\n",
    "# â”€â”€â”€ SCALE FEATURES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "scaler = StandardScaler()\n",
    "X_A_scaled = torch.tensor(scaler.fit_transform(X_A), dtype=torch.float32)\n",
    "X_B_scaled = torch.tensor(scaler.transform(X_B), dtype=torch.float32)\n",
    "\n",
    "# â”€â”€â”€ CLASS DESCRIPTIONS â†’ EMBEDDINGS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class_texts = {\n",
    "    0: \"normal behavior\",\n",
    "    1: \"attack type A\",\n",
    "    2: \"attack type B\"\n",
    "}\n",
    "\n",
    "text_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "class_embeddings = {}\n",
    "for c, text in class_texts.items():\n",
    "    emb = text_encoder.encode(text, convert_to_tensor=True)\n",
    "    class_embeddings[c] = emb / emb.norm()  # normalize\n",
    "\n",
    "# â”€â”€â”€ TIME-SERIES ENCODER MODEL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class TimeSeriesEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, emb_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x / x.norm(dim=-1, keepdim=True)  # normalize\n",
    "\n",
    "# â”€â”€â”€ PREPARE DATA ON DEVICE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "embedding_dim = class_embeddings[0].shape[0]\n",
    "\n",
    "# Move model to device\n",
    "model = TimeSeriesEncoder(input_dim=14, emb_dim=embedding_dim).to(device)\n",
    "\n",
    "# Move data to device\n",
    "X_A_tensor = X_A_scaled.to(device)\n",
    "y_A_tensor = y_A.to(device)\n",
    "X_B_tensor = X_B_scaled.to(device)\n",
    "y_B_tensor = y_B.to(device)\n",
    "\n",
    "# Move class embeddings to device\n",
    "for c in class_embeddings:\n",
    "    class_embeddings[c] = class_embeddings[c].to(device)\n",
    "\n",
    "# â”€â”€â”€ TRAINING LOOP ON DATASET A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    time_emb = model(X_A_tensor)\n",
    "    \n",
    "    class_emb_batch = torch.stack([class_embeddings[label.item()] for label in y_A_tensor])\n",
    "    \n",
    "    cos_sim = (time_emb * class_emb_batch).sum(dim=-1)\n",
    "    loss = -cos_sim.mean()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# â”€â”€â”€ ZERO-SHOT EVALUATION ON DATASET B â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    time_emb_B = model(X_B_tensor)\n",
    "    \n",
    "    # Similarity to all classes (0,1,2)\n",
    "    all_class_emb = torch.stack([class_embeddings[c] for c in [0, 1, 2]])\n",
    "    sims = torch.matmul(time_emb_B, all_class_emb.T)\n",
    "    \n",
    "    preds = sims.argmax(dim=1)\n",
    "    pred_labels = torch.tensor([ [0,1,2][p] for p in preds ]).to(device)\n",
    "    \n",
    "    acc = (pred_labels == y_B_tensor).float().mean().item()\n",
    "    print(f\"\\nZero-Shot Accuracy on Dataset B (with unseen class 2!): {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92e7a9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS available: True\n",
      "MPS built: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "print(\"MPS built:\", torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0d4a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ld/f2yvzx4155lc1np6yd4rhdrm0000gn/T/ipykernel_25991/3702346080.py:9: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "\n",
      "=== Training on Domain A ===\n",
      "[Domain A] Epoch 5, Loss: -0.1406\n",
      "[Domain A] Epoch 10, Loss: -0.2821\n",
      "\n",
      "=== Continual Learning on Domain B ===\n",
      "[Domain B] Epoch 5, Loss: -0.4113\n",
      "[Domain B] Epoch 10, Loss: -0.5039\n",
      "\n",
      "=== Zero-Shot Evaluation on Domain C ===\n",
      "\n",
      "[Domain C] Zero-Shot Accuracy on classes [0, 1, 2]: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "\n",
    "# â”€â”€â”€ DEVICE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# â”€â”€â”€ DATA SIMULATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Domain A â†’ attack X\n",
    "X_A = torch.randn(1000, 14)\n",
    "y_A = torch.randint(0, 2, (1000,))  # 0 = normal, 1 = attack X\n",
    "\n",
    "# Domain B â†’ attack Y\n",
    "X_B = torch.randn(1000, 14) + 1.0\n",
    "y_B = torch.randint(0, 2, (1000,))  # 0 = normal, 1 = attack Y\n",
    "\n",
    "# Domain C â†’ attack Z â†’ Zero-Shot â†’ No samples of class 1 seen in training!\n",
    "X_C = torch.randn(500, 14) + 2.0\n",
    "y_C = torch.randint(0, 2, (500,))  # 0 = normal, 1 = attack Z\n",
    "\n",
    "# â”€â”€â”€ FEATURE SCALING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "scaler = StandardScaler()\n",
    "X_A_scaled = torch.tensor(scaler.fit_transform(X_A), dtype=torch.float32)\n",
    "X_B_scaled = torch.tensor(scaler.transform(X_B), dtype=torch.float32)\n",
    "X_C_scaled = torch.tensor(scaler.transform(X_C), dtype=torch.float32)\n",
    "\n",
    "# â”€â”€â”€ CLASS DESCRIPTIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class_texts = {\n",
    "    0: \"normal behavior\",\n",
    "    1: \"port scan attack\",         # attack X (Domain A)\n",
    "    2: \"brute force login attack\"  # attack Z (Domain C - Zero-Shot)\n",
    "}\n",
    "\n",
    "text_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "class_embeddings = {}\n",
    "for c, text in class_texts.items():\n",
    "    emb = text_encoder.encode(text, convert_to_tensor=True)\n",
    "    class_embeddings[c] = emb / emb.norm()\n",
    "\n",
    "# â”€â”€â”€ TIME-SERIES ENCODER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class TimeSeriesEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, emb_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x / x.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# â”€â”€â”€ PREPARE DATA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "embedding_dim = class_embeddings[0].shape[0]\n",
    "model = TimeSeriesEncoder(input_dim=14, emb_dim=embedding_dim).to(device)\n",
    "\n",
    "# Move data to device\n",
    "X_A_tensor = X_A_scaled.to(device)\n",
    "y_A_tensor = y_A.to(device)\n",
    "X_B_tensor = X_B_scaled.to(device)\n",
    "y_B_tensor = y_B.to(device)\n",
    "X_C_tensor = X_C_scaled.to(device)\n",
    "y_C_tensor = y_C.to(device)\n",
    "\n",
    "# Move class embeddings to device\n",
    "for c in class_embeddings:\n",
    "    class_embeddings[c] = class_embeddings[c].to(device)\n",
    "\n",
    "# â”€â”€â”€ REPLAY BUFFER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "replay_buffer = []\n",
    "\n",
    "# â”€â”€â”€ TRAIN FUNCTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def train_on_domain(X, y, domain_name, epochs=50, replay_ratio=0.2):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Build batch with replay buffer\n",
    "        replay_size = int(replay_ratio * len(replay_buffer))\n",
    "        main_indices = random.sample(range(len(X)), 256)\n",
    "        replay_indices = random.sample(range(len(replay_buffer)), replay_size) if replay_size > 0 else []\n",
    "        \n",
    "        # Current domain batch\n",
    "        X_batch = X[main_indices]\n",
    "        y_batch = y[main_indices]\n",
    "        \n",
    "        # Replay batch\n",
    "        if replay_indices:\n",
    "            X_replay = torch.stack([ replay_buffer[i][0] for i in replay_indices ])\n",
    "            y_replay = torch.stack([ replay_buffer[i][1] for i in replay_indices ])\n",
    "            \n",
    "            X_batch = torch.cat([X_batch, X_replay], dim=0)\n",
    "            y_batch = torch.cat([y_batch, y_replay], dim=0)\n",
    "        \n",
    "        # Forward\n",
    "        time_emb = model(X_batch)\n",
    "        class_emb_batch = torch.stack([class_embeddings[int(label.item())] for label in y_batch])\n",
    "        \n",
    "        # Contrastive loss â†’ cosine similarity\n",
    "        cos_sim = (time_emb * class_emb_batch).sum(dim=-1)\n",
    "        loss = -cos_sim.mean()\n",
    "        \n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(f\"[{domain_name}] Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Update replay buffer\n",
    "    for i in range(len(X)):\n",
    "        if len(replay_buffer) < 500:  # Max buffer size\n",
    "            replay_buffer.append( (X[i], y[i]) )\n",
    "        else:\n",
    "            # Replace random old sample\n",
    "            j = random.randint(0, len(replay_buffer)-1)\n",
    "            replay_buffer[j] = (X[i], y[i])\n",
    "\n",
    "# â”€â”€â”€ ZERO-SHOT EVAL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def zero_shot_eval(X, y, domain_name, eval_classes):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        time_emb = model(X)\n",
    "        \n",
    "        all_class_emb = torch.stack([class_embeddings[c] for c in eval_classes])\n",
    "        sims = torch.matmul(time_emb, all_class_emb.T)\n",
    "        \n",
    "        preds = sims.argmax(dim=1)\n",
    "        pred_labels = torch.tensor([ eval_classes[p] for p in preds ]).to(device)\n",
    "        \n",
    "        acc = (pred_labels == y).float().mean().item()\n",
    "        print(f\"\\n[{domain_name}] Zero-Shot Accuracy on classes {eval_classes}: {acc:.4f}\")\n",
    "\n",
    "# â”€â”€â”€ FULL TRAINING PIPELINE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Step 1: Train on Domain A\n",
    "print(\"\\n=== Training on Domain A ===\")\n",
    "train_on_domain(X_A_tensor, y_A_tensor, domain_name=\"Domain A\", epochs=10)\n",
    "\n",
    "# Step 2: Continual Learning â†’ Domain B\n",
    "print(\"\\n=== Continual Learning on Domain B ===\")\n",
    "train_on_domain(X_B_tensor, y_B_tensor, domain_name=\"Domain B\", epochs=10)\n",
    "\n",
    "# Step 3: Zero-Shot Inference on Domain C â†’ unseen attack Z\n",
    "print(\"\\n=== Zero-Shot Evaluation on Domain C ===\")\n",
    "zero_shot_eval(X_C_tensor, y_C_tensor, domain_name=\"Domain C\", eval_classes=[0, 1, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ced01c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ld/f2yvzx4155lc1np6yd4rhdrm0000gn/T/ipykernel_25991/400237632.py:9: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "\n",
      "=== Training on Domain A ===\n",
      "[A] Epoch 5, Loss: 0.6901\n",
      "[A] Epoch 10, Loss: 0.6935\n",
      "[A] Epoch 15, Loss: 0.6884\n",
      "[A] Epoch 20, Loss: 0.6865\n",
      "[A] Epoch 25, Loss: 0.6758\n",
      "[A] Epoch 30, Loss: 0.6845\n",
      "[A] Epoch 35, Loss: 0.6755\n",
      "[A] Epoch 40, Loss: 0.6687\n",
      "[A] Epoch 45, Loss: 0.6725\n",
      "[A] Epoch 50, Loss: 0.6653\n",
      "[A] Epoch 55, Loss: 0.6475\n",
      "[A] Epoch 60, Loss: 0.6459\n",
      "[A] Epoch 65, Loss: 0.6607\n",
      "[A] Epoch 70, Loss: 0.6493\n",
      "[A] Epoch 75, Loss: 0.6260\n",
      "[A] Epoch 80, Loss: 0.6218\n",
      "[A] Epoch 85, Loss: 0.6021\n",
      "[A] Epoch 90, Loss: 0.6018\n",
      "[A] Epoch 95, Loss: 0.6099\n",
      "[A] Epoch 100, Loss: 0.5994\n",
      "[A] Epoch 105, Loss: 0.6028\n",
      "[A] Epoch 110, Loss: 0.5896\n",
      "[A] Epoch 115, Loss: 0.5989\n",
      "[A] Epoch 120, Loss: 0.5810\n",
      "[A] Epoch 125, Loss: 0.5751\n",
      "[A] Epoch 130, Loss: 0.5683\n",
      "[A] Epoch 135, Loss: 0.5483\n",
      "[A] Epoch 140, Loss: 0.5430\n",
      "[A] Epoch 145, Loss: 0.5729\n",
      "[A] Epoch 150, Loss: 0.5549\n",
      "[A] Epoch 155, Loss: 0.5231\n",
      "[A] Epoch 160, Loss: 0.5537\n",
      "[A] Epoch 165, Loss: 0.5392\n",
      "[A] Epoch 170, Loss: 0.5593\n",
      "[A] Epoch 175, Loss: 0.5205\n",
      "[A] Epoch 180, Loss: 0.5579\n",
      "[A] Epoch 185, Loss: 0.4970\n",
      "[A] Epoch 190, Loss: 0.4959\n",
      "[A] Epoch 195, Loss: 0.4996\n",
      "[A] Epoch 200, Loss: 0.5101\n",
      "[A] Epoch 205, Loss: 0.5079\n",
      "[A] Epoch 210, Loss: 0.4891\n",
      "[A] Epoch 215, Loss: 0.4845\n",
      "[A] Epoch 220, Loss: 0.5034\n",
      "[A] Epoch 225, Loss: 0.4560\n",
      "[A] Epoch 230, Loss: 0.4558\n",
      "[A] Epoch 235, Loss: 0.4689\n",
      "[A] Epoch 240, Loss: 0.4698\n",
      "[A] Epoch 245, Loss: 0.4688\n",
      "[A] Epoch 250, Loss: 0.4539\n",
      "[A] Epoch 255, Loss: 0.4654\n",
      "[A] Epoch 260, Loss: 0.4524\n",
      "[A] Epoch 265, Loss: 0.4245\n",
      "[A] Epoch 270, Loss: 0.4518\n",
      "[A] Epoch 275, Loss: 0.4346\n",
      "[A] Epoch 280, Loss: 0.4209\n",
      "[A] Epoch 285, Loss: 0.4114\n",
      "[A] Epoch 290, Loss: 0.4321\n",
      "[A] Epoch 295, Loss: 0.4143\n",
      "[A] Epoch 300, Loss: 0.4389\n",
      "[A] Epoch 305, Loss: 0.4466\n",
      "[A] Epoch 310, Loss: 0.4367\n",
      "[A] Epoch 315, Loss: 0.4202\n",
      "[A] Epoch 320, Loss: 0.4027\n",
      "[A] Epoch 325, Loss: 0.4385\n",
      "[A] Epoch 330, Loss: 0.4445\n",
      "[A] Epoch 335, Loss: 0.4050\n",
      "[A] Epoch 340, Loss: 0.4406\n",
      "[A] Epoch 345, Loss: 0.4053\n",
      "[A] Epoch 350, Loss: 0.3807\n",
      "[A] Epoch 355, Loss: 0.4120\n",
      "[A] Epoch 360, Loss: 0.3875\n",
      "[A] Epoch 365, Loss: 0.4171\n",
      "[A] Epoch 370, Loss: 0.4110\n",
      "[A] Epoch 375, Loss: 0.4169\n",
      "[A] Epoch 380, Loss: 0.3901\n",
      "[A] Epoch 385, Loss: 0.3819\n",
      "[A] Epoch 390, Loss: 0.3628\n",
      "[A] Epoch 395, Loss: 0.3510\n",
      "[A] Epoch 400, Loss: 0.3826\n",
      "[A] Epoch 405, Loss: 0.3674\n",
      "[A] Epoch 410, Loss: 0.3808\n",
      "[A] Epoch 415, Loss: 0.3829\n",
      "[A] Epoch 420, Loss: 0.3692\n",
      "[A] Epoch 425, Loss: 0.3854\n",
      "[A] Epoch 430, Loss: 0.4318\n",
      "[A] Epoch 435, Loss: 0.3525\n",
      "[A] Epoch 440, Loss: 0.4028\n",
      "[A] Epoch 445, Loss: 0.3683\n",
      "[A] Epoch 450, Loss: 0.3940\n",
      "[A] Epoch 455, Loss: 0.3602\n",
      "[A] Epoch 460, Loss: 0.4036\n",
      "[A] Epoch 465, Loss: 0.3568\n",
      "[A] Epoch 470, Loss: 0.3714\n",
      "[A] Epoch 475, Loss: 0.3705\n",
      "[A] Epoch 480, Loss: 0.4085\n",
      "[A] Epoch 485, Loss: 0.3913\n",
      "[A] Epoch 490, Loss: 0.3695\n",
      "[A] Epoch 495, Loss: 0.3780\n",
      "[A] Epoch 500, Loss: 0.3775\n",
      "\n",
      "=== Continual Learning on Domain B ===\n",
      "[B] Epoch 5, Loss: 0.6462\n",
      "[B] Epoch 10, Loss: 0.6631\n",
      "[B] Epoch 15, Loss: 0.6281\n",
      "[B] Epoch 20, Loss: 0.6493\n",
      "[B] Epoch 25, Loss: 0.5982\n",
      "[B] Epoch 30, Loss: 0.6156\n",
      "[B] Epoch 35, Loss: 0.6085\n",
      "[B] Epoch 40, Loss: 0.6163\n",
      "[B] Epoch 45, Loss: 0.5877\n",
      "[B] Epoch 50, Loss: 0.5967\n",
      "[B] Epoch 55, Loss: 0.5967\n",
      "[B] Epoch 60, Loss: 0.6046\n",
      "[B] Epoch 65, Loss: 0.5687\n",
      "[B] Epoch 70, Loss: 0.5850\n",
      "[B] Epoch 75, Loss: 0.5786\n",
      "[B] Epoch 80, Loss: 0.6072\n",
      "[B] Epoch 85, Loss: 0.5776\n",
      "[B] Epoch 90, Loss: 0.5733\n",
      "[B] Epoch 95, Loss: 0.5708\n",
      "[B] Epoch 100, Loss: 0.5707\n",
      "[B] Epoch 105, Loss: 0.5676\n",
      "[B] Epoch 110, Loss: 0.5358\n",
      "[B] Epoch 115, Loss: 0.5587\n",
      "[B] Epoch 120, Loss: 0.5843\n",
      "[B] Epoch 125, Loss: 0.5811\n",
      "[B] Epoch 130, Loss: 0.5355\n",
      "[B] Epoch 135, Loss: 0.5612\n",
      "[B] Epoch 140, Loss: 0.5035\n",
      "[B] Epoch 145, Loss: 0.5623\n",
      "[B] Epoch 150, Loss: 0.5627\n",
      "[B] Epoch 155, Loss: 0.5245\n",
      "[B] Epoch 160, Loss: 0.5497\n",
      "[B] Epoch 165, Loss: 0.5480\n",
      "[B] Epoch 170, Loss: 0.5425\n",
      "[B] Epoch 175, Loss: 0.5066\n",
      "[B] Epoch 180, Loss: 0.5230\n",
      "[B] Epoch 185, Loss: 0.5273\n",
      "[B] Epoch 190, Loss: 0.4868\n",
      "[B] Epoch 195, Loss: 0.4888\n",
      "[B] Epoch 200, Loss: 0.5035\n",
      "[B] Epoch 205, Loss: 0.5234\n",
      "[B] Epoch 210, Loss: 0.5466\n",
      "[B] Epoch 215, Loss: 0.4830\n",
      "[B] Epoch 220, Loss: 0.5386\n",
      "[B] Epoch 225, Loss: 0.5027\n",
      "[B] Epoch 230, Loss: 0.4975\n",
      "[B] Epoch 235, Loss: 0.4761\n",
      "[B] Epoch 240, Loss: 0.5040\n",
      "[B] Epoch 245, Loss: 0.5055\n",
      "[B] Epoch 250, Loss: 0.5039\n",
      "[B] Epoch 255, Loss: 0.4936\n",
      "[B] Epoch 260, Loss: 0.4650\n",
      "[B] Epoch 265, Loss: 0.4718\n",
      "[B] Epoch 270, Loss: 0.4782\n",
      "[B] Epoch 275, Loss: 0.5087\n",
      "[B] Epoch 280, Loss: 0.5199\n",
      "[B] Epoch 285, Loss: 0.4820\n",
      "[B] Epoch 290, Loss: 0.5055\n",
      "[B] Epoch 295, Loss: 0.5124\n",
      "[B] Epoch 300, Loss: 0.4870\n",
      "[B] Epoch 305, Loss: 0.4661\n",
      "[B] Epoch 310, Loss: 0.4830\n",
      "[B] Epoch 315, Loss: 0.4996\n",
      "[B] Epoch 320, Loss: 0.4839\n",
      "[B] Epoch 325, Loss: 0.4872\n",
      "[B] Epoch 330, Loss: 0.4938\n",
      "[B] Epoch 335, Loss: 0.5253\n",
      "[B] Epoch 340, Loss: 0.5108\n",
      "[B] Epoch 345, Loss: 0.4837\n",
      "[B] Epoch 350, Loss: 0.4923\n",
      "[B] Epoch 355, Loss: 0.4732\n",
      "[B] Epoch 360, Loss: 0.5091\n",
      "[B] Epoch 365, Loss: 0.5506\n",
      "[B] Epoch 370, Loss: 0.5113\n",
      "[B] Epoch 375, Loss: 0.5130\n",
      "[B] Epoch 380, Loss: 0.4839\n",
      "[B] Epoch 385, Loss: 0.4799\n",
      "[B] Epoch 390, Loss: 0.4910\n",
      "[B] Epoch 395, Loss: 0.4845\n",
      "[B] Epoch 400, Loss: 0.5118\n",
      "[B] Epoch 405, Loss: 0.4708\n",
      "[B] Epoch 410, Loss: 0.4615\n",
      "[B] Epoch 415, Loss: 0.4700\n",
      "[B] Epoch 420, Loss: 0.4620\n",
      "[B] Epoch 425, Loss: 0.4717\n",
      "[B] Epoch 430, Loss: 0.5070\n",
      "[B] Epoch 435, Loss: 0.4764\n",
      "[B] Epoch 440, Loss: 0.4839\n",
      "[B] Epoch 445, Loss: 0.4596\n",
      "[B] Epoch 450, Loss: 0.4481\n",
      "[B] Epoch 455, Loss: 0.5153\n",
      "[B] Epoch 460, Loss: 0.4860\n",
      "[B] Epoch 465, Loss: 0.4766\n",
      "[B] Epoch 470, Loss: 0.4608\n",
      "[B] Epoch 475, Loss: 0.4578\n",
      "[B] Epoch 480, Loss: 0.4924\n",
      "[B] Epoch 485, Loss: 0.4826\n",
      "[B] Epoch 490, Loss: 0.4890\n",
      "[B] Epoch 495, Loss: 0.4574\n",
      "[B] Epoch 500, Loss: 0.4777\n",
      "\n",
      "=== Zero-Shot Evaluation on Domain C ===\n",
      "\n",
      "[C] Accuracy on classes [0, 1]: 0.4880\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "\n",
    "# â”€â”€â”€ DEVICE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# â”€â”€â”€ DATA SIMULATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Domain A â†’ attack X\n",
    "X_A = torch.randn(1000, 14)\n",
    "y_A = torch.randint(0, 2, (1000,))  # 0 = normal, 1 = attack X\n",
    "\n",
    "# Domain B â†’ attack Y\n",
    "X_B = torch.randn(1000, 14) + 1.0\n",
    "y_B = torch.randint(0, 2, (1000,))  # 0 = normal, 1 = attack Y\n",
    "\n",
    "# Domain C â†’ attack Z â†’ Now also class 1 â†’ ZSCL (no training!)\n",
    "X_C = torch.randn(500, 14) + 2.0\n",
    "y_C = torch.randint(0, 2, (500,))  # 0 = normal, 1 = attack Z\n",
    "\n",
    "# â”€â”€â”€ FEATURE SCALING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "scaler = StandardScaler()\n",
    "X_A_scaled = torch.tensor(scaler.fit_transform(X_A), dtype=torch.float32)\n",
    "X_B_scaled = torch.tensor(scaler.transform(X_B), dtype=torch.float32)\n",
    "X_C_scaled = torch.tensor(scaler.transform(X_C), dtype=torch.float32)\n",
    "\n",
    "# â”€â”€â”€ CLASS DESCRIPTIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class_texts = {\n",
    "    0: \"normal behavior\",\n",
    "    1: \"attack behavior\"\n",
    "}\n",
    "\n",
    "text_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "class_embeddings = {}\n",
    "for c, text in class_texts.items():\n",
    "    emb = text_encoder.encode(text, convert_to_tensor=True)\n",
    "    class_embeddings[c] = emb / emb.norm()\n",
    "\n",
    "# â”€â”€â”€ DOMAIN EMBEDDINGS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "domain_texts = {\n",
    "    \"A\": \"domain A\",\n",
    "    \"B\": \"domain B\",\n",
    "    \"C\": \"domain C\"\n",
    "}\n",
    "\n",
    "domain_embeddings = {}\n",
    "for d, text in domain_texts.items():\n",
    "    emb = text_encoder.encode(text, convert_to_tensor=True)\n",
    "    domain_embeddings[d] = emb / emb.norm()\n",
    "\n",
    "# â”€â”€â”€ DOMAIN-CONDITIONED TIME-SERIES ENCODER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class DomainConditionedEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, domain_emb_dim, output_emb_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim + domain_emb_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_emb_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, domain_emb):\n",
    "        domain_emb_expanded = domain_emb.unsqueeze(0).repeat(len(x), 1)\n",
    "        x_cat = torch.cat([x, domain_emb_expanded], dim=1)\n",
    "        x_out = self.net(x_cat)\n",
    "        return x_out / x_out.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# â”€â”€â”€ PREPARE DATA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "embedding_dim = class_embeddings[0].shape[0]\n",
    "domain_emb_dim = domain_embeddings[\"A\"].shape[0]\n",
    "\n",
    "model = DomainConditionedEncoder(input_dim=14, domain_emb_dim=domain_emb_dim, output_emb_dim=embedding_dim).to(device)\n",
    "\n",
    "# Move data to device\n",
    "X_A_tensor = X_A_scaled.to(device)\n",
    "y_A_tensor = y_A.to(device)\n",
    "X_B_tensor = X_B_scaled.to(device)\n",
    "y_B_tensor = y_B.to(device)\n",
    "X_C_tensor = X_C_scaled.to(device)\n",
    "y_C_tensor = y_C.to(device)\n",
    "\n",
    "# Move class embeddings to device\n",
    "for c in class_embeddings:\n",
    "    class_embeddings[c] = class_embeddings[c].to(device)\n",
    "\n",
    "# Move domain embeddings to device\n",
    "for d in domain_embeddings:\n",
    "    domain_embeddings[d] = domain_embeddings[d].to(device)\n",
    "\n",
    "# â”€â”€â”€ REPLAY BUFFER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "replay_buffer = []\n",
    "\n",
    "# â”€â”€â”€ TRAIN FUNCTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def train_on_domain(X, y, domain_name, epochs=50, replay_ratio=0.2):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    domain_emb = domain_embeddings[domain_name]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Build batch with replay buffer\n",
    "        replay_size = int(replay_ratio * len(replay_buffer))\n",
    "        main_indices = random.sample(range(len(X)), 256)\n",
    "        replay_indices = random.sample(range(len(replay_buffer)), replay_size) if replay_size > 0 else []\n",
    "        \n",
    "        # Current domain batch\n",
    "        X_batch = X[main_indices]\n",
    "        y_batch = y[main_indices]\n",
    "        \n",
    "        # Replay batch\n",
    "        if replay_indices:\n",
    "            X_replay = torch.stack([ replay_buffer[i][0] for i in replay_indices ])\n",
    "            y_replay = torch.stack([ replay_buffer[i][1] for i in replay_indices ])\n",
    "            \n",
    "            X_batch = torch.cat([X_batch, X_replay], dim=0)\n",
    "            y_batch = torch.cat([y_batch, y_replay], dim=0)\n",
    "        \n",
    "        # Forward pass\n",
    "        time_emb = model(X_batch, domain_emb)\n",
    "        \n",
    "        all_class_emb = torch.stack([class_embeddings[c] for c in class_texts.keys()])\n",
    "        logits = torch.matmul(time_emb, all_class_emb.T)\n",
    "        \n",
    "        targets = torch.tensor([int(label.item()) for label in y_batch]).to(device)\n",
    "        \n",
    "        ce_loss = nn.CrossEntropyLoss()(logits, targets)\n",
    "        \n",
    "        ce_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(f\"[{domain_name}] Epoch {epoch+1}, Loss: {ce_loss.item():.4f}\")\n",
    "    \n",
    "    # Update replay buffer\n",
    "    for i in range(len(X)):\n",
    "        if len(replay_buffer) < 500:  # Max buffer size\n",
    "            replay_buffer.append( (X[i], y[i]) )\n",
    "        else:\n",
    "            j = random.randint(0, len(replay_buffer)-1)\n",
    "            replay_buffer[j] = (X[i], y[i])\n",
    "\n",
    "# â”€â”€â”€ EVALUATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def evaluate(X, y, domain_name, eval_classes):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        domain_emb = domain_embeddings[domain_name]\n",
    "        time_emb = model(X, domain_emb)\n",
    "        \n",
    "        all_class_emb = torch.stack([class_embeddings[c] for c in eval_classes])\n",
    "        sims = torch.matmul(time_emb, all_class_emb.T)\n",
    "        \n",
    "        preds = sims.argmax(dim=1)\n",
    "        pred_labels = torch.tensor([ eval_classes[p] for p in preds ]).to(device)\n",
    "        \n",
    "        acc = (pred_labels == y).float().mean().item()\n",
    "        print(f\"\\n[{domain_name}] Accuracy on classes {eval_classes}: {acc:.4f}\")\n",
    "\n",
    "# â”€â”€â”€ FULL ZSCL PIPELINE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Step 1: Train on Domain A\n",
    "print(\"\\n=== Training on Domain A ===\")\n",
    "train_on_domain(X_A_tensor, y_A_tensor, domain_name=\"A\", epochs=500)\n",
    "\n",
    "# Step 2: Continual Learning â†’ Domain B\n",
    "print(\"\\n=== Continual Learning on Domain B ===\")\n",
    "train_on_domain(X_B_tensor, y_B_tensor, domain_name=\"B\", epochs=500)\n",
    "\n",
    "# ðŸš« ðŸš« ðŸš« IMPORTANT: NO training on Domain C ðŸš« ðŸš« ðŸš«\n",
    "# We skip:\n",
    "# train_on_domain(X_C_tensor, y_C_tensor, domain_name=\"C\", epochs=10)\n",
    "\n",
    "# Step 3: Zero-Shot Evaluation on unseen Domain C!\n",
    "print(\"\\n=== Zero-Shot Evaluation on Domain C ===\")\n",
    "evaluate(X_C_tensor, y_C_tensor, domain_name=\"C\", eval_classes=[0, 1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vinnova",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
